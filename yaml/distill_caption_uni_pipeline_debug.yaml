all_test_data:
- test_data: TaxCocoCaptionSmall
  test_split: train
param:
    data: TaxCocoCaptionSmall
    drop_out: 0
    net: B
    mask_type: seq2seq
    tokenizer_file: ./yaml/vinvl_label.json
    topk: 50
    train_shuffle: False
    basemodel: ./output/Jacob_Tagger_TaxCCSBUCocoVGCap_B_Vilt_ViT_16_384_20_epoch_lr_5e-5_BS_1024_loss_focal_crop_0.08_bert_category/model_final.pt
    clip_checkpoint: /home/ubuntu/.cache/clip/ViT-B-16.pt
    text_encoder_type: ./yaml/VILT-L12-H784-uncased_16_384
    teacher_encoder_type: ./data/vinvl/captioning_vinvl_base
    max_img_seq_length: 0
    image_encoder_type: VitEmb_vit_base_patch16_384
    crop_pct: 1.0
    base_lr: 1.0e-04
#    base_lr: 10
    # lr multiplier for the backbone
    lr_multiplier: .0
    split_blocks: 4
    monitor_after: True
    test_crop_size: 384
    train_crop_size: 384
    multi_scale: False
    multi_crop: False
    scale2: 192
    scale3: 96
    multi_crop_scale: False
    num_scale2: 2
    num_scale3: 4
    dist_url_tcp_port: 12345
    train_transform: clip_vit
    use_img_layernorm: False
    expid: TaxCocoCaption_B_Vilt_VinVL_captioning_testing
    full_expid: TaxCocoCaption_B_Vilt_VinVL_captioning_testing
    image_encoder_pretrained: True
    expid_prefix: Jacob_Vilt
    pad_to_max: True
    add_od_labels: True
    # for test when there is no tag tsv
    load_no_label_model: True
    effective_batch_size: 1
    test_batch_size: 1
    max_iter: 30e
    ignore_project_image: True
    input_small_scale: 0.08
    log_step: 100
    weight_decay: 0.05
    num_workers: 0
    expid_prefix: CAPU
    use_amp: False
    max_seq_length: 70
    max_seq_a_length: 20
    max_img_feat_seq_length: 50
    img_feature_dim: 2054
    train_feature_version: vinvl
    # vinvl eff0fpeter eff1f  linvits  vitb
    train_label_version: vinvl
    find_unused_parameters: True
    output_hidden_states: False
    force_train: True
    diff_token_sample: 1.0
    kemans_token_sample: 1.0
    kmeans_centroid: False
    k_means_visualize: False
    random_token_sample: 0.1
    token_diff_token_sample: 1.0
    train_random_sample_only: True
    return_attention: False
    self_sample_rate: 1.0
    od_label_conf: 0.2
    ce_weight: 1
    logit_weight: 10
    force_predict: True
#   README: category is for 2k or 30k tags;  tagemb is for use classifier's embedding or tokenizer's embedding;
#           encode is for to use NLTK keyword or BERT all tokens. if caption_only is True, use no tags but just captions for tag prediction.
    caption_only: False
    loss: focal
    topktagger: False
    # bert or cls
    tagemb: bert
    tagemb_gradient: False
    # how to tokenize the caption into label, bert or nltk
    encode: nltk
    # vinvl or bert
    category: bert
    gen_tag_ratio: 0.05
    gt_tag_train: False
    tie_tag_weights: False
    scst: True
    use_cbs: False
    cbs_box: ./data/nocaps/cbs/val.cbsbox.tsv
    cbs_constraint: ./data/nocaps/cbs/constraint_to_tokens.tsv
    cbs_tokenforms: ./data/nocaps/cbs/constraint_tokenforms.tsv
    cbs_hierarchy: ./data/nocaps/cbs/class_hierarchy.json
    scst_num_return: 1
    pipeline_type:
         from: src.pipelines.tagger_caption_uni_pipeline_expanding_bertemb
         import: CaptionUniPipeline
#type: pipeline_train_eval_multi pipeline_eval_multi
type: pipeline_train_eval_multi

