# ViTCAP

 This repo contains the code for CVPR-2022 paper [Injecting Semantic Concepts into End-to-End Image Captioning](https://arxiv.org/abs/2112.05230).

 <img src="images/ViTCAP.png" width="650"> 

 ViTCAP is an end-to-end transformer-based image captioning model. ViTCAP takes the raw images as input and predictL 1. Semantic concepts exist in the image, and 2. a open-form textual description of the image. This repo contains the implementation and evaluation of ViTCAP on COCO-captioning dataset.
 
 
 ## ViTCAP Demo
  
  Please see [Loading Script.ipynb](Loading%20Script.ipynb) for a quick demo to load a trained ViTCAP checkpoint for inference.
  
 
 ## Dependencies
  The project enviroment in my local is PyTorch 1.6.
  [requirements.txt](requirements.txt) contains the dependent packages in python3. Since this requirements.txt is auto-generated by conda, <ins>it contains much more unnecessary packages than actual needed.</in>
  

 ## Dataset
  
 Download the [COCO-captioning TSV files]() and place it in `./data/coco_caption`.  For the VL pre-training corpus of ViTCAP. refer to [VinVL Repo](https://github.com/microsoft/Oscar/blob/master/VinVL_DOWNLOAD.md) for downloading large scale VL corpus.

 ## Training & Evaluation
 The bellowing commands will do the training & evaluation.
 
 ## Checkpoint
 
 Download the COCO-caption pre-trained checkpoint [here]().
 
    
 ## ToDo
- [x] Training and evaluation code
- [x] Quick demo notebook code.
- [x] COCO training TSV file.
- [x] COCO pre-trained checkpoint.
- [ ] Visualization codes for predicted semantic concepts and grounded visual concepts.

    VLP pre-trained checkpoint is not planned to be released at this time.
            
 
 ## Citation
  
 Please cite our work if you find it helpful:
  
```bibtex
@inproceedings{fang2021injecting,
title={Injecting Semantic Concepts into End-to-End Image Captioning},
author={Zhiyuan Fang, Jianfeng Wang, Xiaowei Hu, Lin Liang, Zhe Gan, Lijuan Wang, Yezhou Yang, Zicheng Liu},
booktitle = {CVPR},
year = {2022},
}
```

## Acknowledgments
This implementation is largely based on [Jianfeng Wang](https://www.linkedin.com/in/jianfengwang1/)'s efforts and [Microsoft Azure-Florence Group](https://www.microsoft.com/en-us/research/project/project-florence-vl/). Thanks my collaborators.


## License
ViTCAP is released under the MIT license.


